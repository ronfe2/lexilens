# Technical Specification – Respond Speed Optimization

## 1. Technical Context

- **Backend**
  - FastAPI app in `backend/app/main.py`, SSE via `sse_starlette.EventSourceResponse`.
  - LLM orchestration in `backend/app/services/llm_orchestrator.py`.
  - OpenRouter HTTP client in `backend/app/services/openrouter.py`.
  - Prompt templates in `backend/app/prompt_config.py`.
  - Models in `backend/app/models/{request,response,interests}.py`.
  - Streaming helper in `backend/app/utils/streaming.py`.
  - Tests in `backend/tests/`.
- **Frontend (Chrome extension)**
  - React + Vite + TypeScript under `extension/src`.
  - Core streaming hook: `extension/src/sidepanel/hooks/useStreamingAnalysis.ts`.
  - Main UI: `extension/src/sidepanel/App.tsx`.
  - Key components:
    - 解读: `CoachSummary.tsx` (uses `analysisResult.layer4.personalizedTip`).
    - Lexical Map text graph + manga image: `CognitiveScaffolding.tsx` (uses `analysisResult.layer4.relatedWords` and `/api/lexical-map/image`).
    - Common Mistakes: `CommonMistakes.tsx` (uses `analysisResult.layer3`).
  - Central store: `extension/src/store/appStore.ts`.
  - Shared types: `extension/src/shared/types.ts`.

## 2. Current Flow & Latency Pain Points

1. **Current backend flow (`POST /api/analyze` SSE)**
   - `AnalyzeRequest` → `LLMOrchestrator.analyze_streaming`.
   - Step 1: `generate_layer1_stream` streams Layer 1 (Cobuild-style definition) chunk by chunk.
     - Emits `layer1_chunk` events via `stream_sse_events`.
     - At end, emits `layer1_complete` with full content.
   - Step 2: after Layer 1 completes:
     - Creates 3 tasks concurrently:
       - `generate_layer2` → 3 `LiveContext` entries (Layer 2).
       - `generate_layer3` → list of `CommonMistake` (Layer 3).
       - `generate_layer4` → `related_words` + `personalized` (Layer 4).
     - **Sequential await bug**: tasks are started concurrently but awaited in fixed order (2, then 3, then 4). A fast Layer 3 or 4 must wait for slower layers before its SSE event can be emitted.
     - Emits `layer2`, `layer3`, `layer4` events (or `*_error`), then `done`.
   - All calls use the same `OpenRouterClient` model id (`settings.openrouter_model_id`, default Sonnet) with JSON-heavy prompts and high `max_tokens` (600–800).

2. **Current frontend flow**
   - `useStreamingAnalysis.startAnalysis`:
     - Starts SSE `POST /api/analyze`.
     - Handles events:
       - `layer1_chunk`/`layer1_complete` → `analysisResult.layer1.definition`.
       - `layer2` → `analysisResult.layer2: LiveContext[]`.
       - `layer3` → `analysisResult.layer3: CommonMistake[]`.
       - `layer4` → `analysisResult.layer4: { relatedWords, personalizedTip }`.
       - Errors → `setError`, `done` → `setLoading(false)`.
     - In parallel, non-blocking `GET /api/pronunciation/{word}`.
   - `App.tsx` renders:
     - `Header` (uses pronunciation + layer1).
     - `CoachSummary` when `analysisResult.layer4?.personalizedTip` exists.
     - `CognitiveScaffolding` when `analysisResult.layer4` exists.
     - `CommonMistakes` when `analysisResult.layer3` exists.
     - A generic loading spinner if `isLoading` is true and any layer is still missing.
   - Lexical Map manga image:
     - `CognitiveScaffolding` calls `POST /api/lexical-map/image` when user clicks “绘制漫画”.
     - For the **Lexi Learner** profile, a warmup background request prefetches the image for a preferred related word using `enableAsyncImageWarmup`.

3. **Identified latency/cost issues**
   - Layer 2–4 generation happens only **after** Layer 1 finishes.
   - Despite concurrent tasks, the **sequential await** pattern delays faster layers.
   - All LLM calls use the same relatively heavy model and prompts.
   - `LiveContext.icon` is generated by the LLM but **unused** in the UI (icons are derived from `source`).
   - Lexical Map currently produces only 2 related words; user requested up to 5 for better recall.
   - 解读, Lexical Map, and Common Mistakes are all generated unconditionally, even if the user never scrolls to those sections.

## 3. Design Goals (Technical)

Aligning with the PRD:

- Reduce **time-to-first meaningful content** for:
  - 解读 (personalized Chinese summary).
  - Lexical Map (at least nodes, ideally also short differences).
  - Common Mistakes (at least 1 item quickly).
- Avoid unnecessary work by **skipping or deferring heavy layers** when not needed.
- Introduce **per-layer model configuration** and optional reasoning/“thinking” flags.
- Preserve **Lexi Learner image warmup** behavior.
- Keep `/api/analyze` SSE **backward compatible** for existing extension versions.

## 4. Backend Specification

### 4.1 SSE streaming improvements (LLMOrchestrator)

Files:
- `backend/app/services/llm_orchestrator.py`
- `backend/app/models/request.py`
- `backend/app/utils/streaming.py`
- `backend/app/api/routes/analyze.py`

#### 4.1.1 Add optional per-request layer selection

- Extend `AnalyzeRequest` with an optional field:

  ```python
  class AnalyzeRequest(BaseModel):
      ...
      layers: Optional[list[int]] = Field(
          default=None,
          description="Optional list of layers (2,3,4) to compute as part of this analyze request. "
                      "Layer 1 is always streamed. When omitted or empty, defaults to [2,3,4].",
      )
  ```

- Behavior in `LLMOrchestrator.analyze_streaming`:
  - Compute `requested_layers: set[int] = set(request.layers or [2, 3, 4])`.
  - **Layer 1** is always generated/streamed as today (FR1).
  - Only create layer tasks for included layers:

    ```python
    tasks: dict[str, asyncio.Task[Any]] = {}
    if 2 in requested_layers:
        tasks["layer2"] = asyncio.create_task(self.generate_layer2(...))
    if 3 in requested_layers:
        tasks["layer3"] = asyncio.create_task(self.generate_layer3(...))
    if 4 in requested_layers:
        tasks["layer4"] = asyncio.create_task(self.generate_layer4(...))
    ```

  - This provides the backend capability to **defer heavy layers** (FR2). The current extension, which does not send `layers`, will continue to receive all layers.

#### 4.1.2 Emit layers as they complete (fix sequential await)

- Replace the current fixed-order loop:

  ```python
  for task, event_name in [(layer2_task, "layer2"), ...]:
      result = await task
      yield {"event": event_name, ...}
  ```

- With completion-order streaming:

  ```python
  pending = {name: task for name, task in tasks.items()}

  while pending:
      done, _ = await asyncio.wait(
          pending.values(), return_when=asyncio.FIRST_COMPLETED
      )
      for finished in done:
          # Find the event name for this task
          event_name = next(
              name for name, t in pending.items() if t is finished
          )
          del pending[event_name]

          try:
              result = await finished
              yield {"event": event_name, "data": result.model_dump()}
              logger.info("%s completed for '%s'", event_name, word)
          except Exception as e:
              logger.error("Error in %s: %s", event_name, e)
              yield {"event": f"{event_name}_error", "data": {"error": str(e)}}
  ```

- This maintains the same event names while letting whichever layer finishes first reach the UI first, directly improving perceived latency for Lexical Map and Common Mistakes.

#### 4.1.3 Keep SSE framing unchanged

- `stream_sse_events` already converts `{"event": ..., "data": ...}` into SSE frames with JSON payloads.
- No changes needed here; existing test `test_stream_sse_events_normalizes_events_to_json_strings` should continue to pass.

### 4.2 New secondary endpoints for deferred layers

To support FR2, FR5, and FR9, add dedicated non-SSE endpoints for Common Mistakes and Lexical Map text data.

#### 4.2.1 Common Mistakes endpoint

Files:
- `backend/app/models/request.py`
- `backend/app/api/routes/analyze.py` (or a new `mistakes.py` route module)

Changes:

- Add a request model:

  ```python
  class CommonMistakesRequest(BaseModel):
      word: str
      context: str
      english_level: Optional[str] = None
  ```

- Expose an endpoint:

  ```python
  @router.post("/analyze/mistakes", response_model=Layer3Response)
  async def generate_common_mistakes(request: CommonMistakesRequest) -> Layer3Response:
      mistakes = await llm_orchestrator.generate_layer3(
          word=request.word,
          context=request.context,
          english_level=request.english_level,
      )
      return mistakes
  ```

- Error handling:
  - Mirror existing patterns (wrap `OpenRouterError`, `RateLimitError`, `APIConnectionError` into HTTP errors with appropriate status codes).

- Use cases:
  - Frontend triggers this endpoint **only when the Common Mistakes section becomes visible or when user clicks a “生成常见错误” button** (FR9).
  - Allows the initial `analyze` SSE call to skip Layer 3 by setting `layers=[2,4]` or `[2]` in the request.

#### 4.2.2 Lexical Map text endpoint (single-stage, extensible to two-stage)

Files:
- `backend/app/models/request.py`
- `backend/app/api/routes/lexical_map.py`
- `backend/app/services/llm_orchestrator.py`

Changes:

- Add a request model for text data:

  ```python
  class LexicalMapTextRequest(BaseModel):
      word: str
      context: str
      learning_history: Optional[list[str]] = Field(default_factory=list)
      english_level: Optional[str] = None
      interests: Optional[list[InterestTopicPayload]] = Field(default_factory=list)
      blocked_titles: Optional[list[str]] = Field(default_factory=list)
      favorite_words: Optional[list[str]] = Field(default_factory=list)
  ```

- Expose an endpoint under the existing router:

  ```python
  @router.post("/lexical-map/text", response_model=Layer4Response)
  async def generate_lexical_map_text(
      request: LexicalMapTextRequest,
  ) -> Layer4Response:
      result = await llm_orchestrator.generate_layer4(
          word=request.word,
          context=request.context,
          learning_history=request.learning_history,
          english_level=request.english_level,
          interests=request.interests,
          blocked_titles=request.blocked_titles,
          favorite_words=request.favorite_words,
      )
      return result
  ```

- Initial iteration: reuse `generate_layer4` as-is so we have a straightforward non-SSE entry point.
- This endpoint will later be used by the frontend to:
  - Lazily load Lexical Map and 解读 when the section scrolls into view.
  - Support a two-stage Lexical Map strategy (see 4.3.2).

### 4.3 Lexical Map & 解读 (Layer 4) behavior

Files:
- `backend/app/services/llm_orchestrator.py`
- `backend/app/prompt_config.py`
- `extension/src/shared/types.ts`
- `extension/src/sidepanel/hooks/useStreamingAnalysis.ts`
- `extension/src/components/{CognitiveScaffolding,CoachSummary}.tsx`

#### 4.3.1 Increase related word count to up to 5

- Prompt change in `PROMPT_CONFIG["layer4"]["user_prompt_template"]`:
  - Update text from “Recommend 2 related words/phrases” to “Recommend up to 5 related words/phrases”.
  - Clarify that **quality is more important than quantity** and that returning fewer than 5 is acceptable when appropriate.

- Backend parsing (`generate_layer4`):

  ```python
  related_words_data = response.get("related_words", [])
  if not isinstance(related_words_data, list) or len(related_words_data) < 1:
      raise OpenRouterError("Layer 4 must contain at least 1 related word")

  # Cap at 5 for performance, even if the model returns more.
  max_related = 5
  related_words = []
  for item in related_words_data[:max_related]:
      related_words.append(RelatedWord(...))
  ```

- Frontend:
  - `CognitiveScaffolding` currently lays out 4 nodes (`positions` array length 4) and slices `data.relatedWords.slice(0, positions.length)`.
  - Keep `positions` at 4 for now (for visual simplicity); extra candidates remain available for:
    - Future UI features.
    - Choosing the “best” 4 nodes (e.g., synonyms + favorites).

#### 4.3.2 Prepare for two-stage Lexical Map strategy

Goal (FR5): small/fast model recalls up to 5 candidates; standard model adds rich labels and personalized coaching.

Implementation (first iteration – backend only, UI still sees a single `Layer4Response`):

- Add new internal methods to `LLMOrchestrator`:

  ```python
  async def generate_layer4_candidates(..., model_id: Optional[str] = None) -> list[RelatedWord]:
      # Uses a lighter prompt and smaller max_tokens, with a fast model.

  async def enrich_layer4_from_candidates(
      self,
      word: str,
      context: str,
      candidates: list[RelatedWord],
      learning_history: list[str] | None,
      english_level: str | None,
      interests: Optional[list[InterestTopicPayload]],
      blocked_titles: Optional[list[str]],
      favorite_words: Optional[list[str]],
      model_id: Optional[str] = None,
  ) -> Layer4Response:
      # Uses the main model to add difference/when_to_use and personalized coaching.
  ```

- `generate_layer4` becomes an orchestration wrapper:

  ```python
  async def generate_layer4(... ) -> Layer4Response:
      candidates = await self.generate_layer4_candidates(
          word, context, ..., model_id=model_config.layer4_fast_model
      )
      return await self.enrich_layer4_from_candidates(
          word, context, candidates, ..., model_id=model_config.layer4_main_model
      )
  ```

- Prompt side:
  - Add `PROMPT_CONFIG["layer4_candidates"]` for Stage A (word + relationship only, very short descriptions).
  - Reuse/extend `PROMPT_CONFIG["layer4"]` for Stage B, taking candidate list as input.

- Frontend impact in this iteration:
  - None: API shape of `Layer4Response` stays the same.
  - Latency may be similar or slightly higher until we switch front-end to a more incremental usage pattern.

Future extension (not necessarily in this iteration, but enabled by this design):
- SSE and/or `/lexical-map/text` endpoint can:
  - Emit candidate-only related words quickly (nodes only).
  - Fill in `difference`/`when_to_use` and `personalized` once the slower Stage B completes.

#### 4.3.3 解读 (personalized coaching) latency optimizations

- Keep `personalized` within Layer 4, but:
  - Tighten the prompt to emphasize concise 1–3 sentence output.
  - Reduce `max_tokens` for `generate_layer4` (e.g., 800 → 600) while monitoring quality.
  - Optionally use a `thinking`/reasoning flag only in Stage B if experiments show benefit; otherwise keep it disabled for speed (FR13).

- In a later front-end iteration, we can:
  - Allow `CoachSummary` to show a skeleton card (“正在为你整理解读…”) while `personalized` is loading.
  - Optionally provide a “点击刷新解读” button that re-calls `/lexical-map/text` with updated interests/favorites.

### 4.4 Common Mistakes (Layer 3) latency tuning

Files:
- `backend/app/services/llm_orchestrator.py`
- `backend/app/prompt_config.py`
- `extension/src/components/CommonMistakes.tsx`
- `extension/src/sidepanel/hooks/useStreamingAnalysis.ts`

Backend:
- `generate_layer3` currently:
  - Calls `client.complete_json` with `max_tokens=600`.
  - Requires list length ≥ 1 and slices `response[:2]`.
- Planned changes:
  - Emphasize in prompt that each mistake explanation should be short (1–2 Chinese sentences).
  - Reduce `max_tokens` (e.g., 600 → 400) and rely on stricter prompt wording to keep answers compact.
  - Add an internal option to request a single mistake (for future experiments):

    ```python
    async def generate_layer3(..., max_items: int = 2) -> Layer3Response:
        ...
        for item in response[:max_items]:
            ...
    ```

- The `/analyze/mistakes` endpoint can use `max_items=2` now and be tuned later.

Frontend (initially):
- Continue to consume `layer3` SSE event as today.
- Later, when we move to deferred loading:
  - Skip Layer 3 in the initial SSE request by sending `layers=[2,4]`.
  - Have `CommonMistakes` trigger the new `/api/analyze/mistakes` endpoint on mount or on user click, and show a loading skeleton meanwhile (FR8, FR9).

### 4.5 Avoiding unused data

Files:
- `backend/app/services/llm_orchestrator.py`
- `backend/app/prompt_config.py`
- `backend/app/models/response.py`
- `extension/src/components/LiveContexts.tsx`

Planned change (FR10):

- **Layer 2 `icon` field**:
  - `LiveContext` model already has optional `icon`, but the frontend uses `source` to pick icons.
  - Update `PROMPT_CONFIG["layer2"]` to no longer mention/require an `icon` field.
  - In `generate_layer2`, stop reading `icon` from the LLM response; always pass `icon=None` to `LiveContext` (or omit entirely via default).

- This reduces token usage in the Layer 2 JSON schema and simplifies the LLM task, with no UI impact.

### 4.6 Per-layer model configuration & “thinking” flags

Files:
- `backend/app/config.py`
- `backend/app/services/llm_orchestrator.py`

Config:

- Extend `Settings`:

  ```python
  class Settings(BaseSettings):
      ...
      openrouter_model_id: str = "anthropic/claude-3.5-sonnet"
      openrouter_fast_model_id: Optional[str] = None
      openrouter_layer3_model_id: Optional[str] = None
      openrouter_layer4_fast_model_id: Optional[str] = None
      openrouter_layer4_main_model_id: Optional[str] = None

      # Optional flags for vendor-specific reasoning/thinking modes
      openrouter_layer3_thinking_enabled: bool = False
      openrouter_layer4_thinking_enabled: bool = False
  ```

- Mapping inside `LLMOrchestrator`:

  ```python
  class LLMOrchestrator:
      def __init__(self):
          self.client = openrouter_client

      def _model_for(self, layer: str) -> str:
          # Fall back to global default when no layer-specific override is set.
          ...
  ```

- Usage examples:
  - `generate_layer2`: `await self.client.complete_json(..., model=settings.openrouter_fast_model_id or settings.openrouter_model_id)`
  - `generate_layer3`: use `settings.openrouter_layer3_model_id` if set.
  - `generate_layer4_candidates`: use `settings.openrouter_layer4_fast_model_id or settings.openrouter_fast_model_id`.
  - `generate_layer4`/Stage B: use `settings.openrouter_layer4_main_model_id or settings.openrouter_model_id`.

- Thinking flags:
  - We will pass any vendor-specific parameters via `**kwargs` to `complete`/`complete_json` (e.g., `thinking=True` or `reasoning_effort="medium"`), controlled by the `*_thinking_enabled` flags.
  - Default is **off** for speed; can be enabled per-layer without code changes (FR12–FR14).

## 5. Frontend Specification

Files:
- `extension/src/shared/types.ts`
- `extension/src/sidepanel/hooks/useStreamingAnalysis.ts`
- `extension/src/sidepanel/App.tsx`
- `extension/src/components/{CoachSummary,CognitiveScaffolding,CommonMistakes}.tsx`

### 5.1 Types & API payloads

- `AnalysisRequest`:
  - Add optional `layers?: number[]` field mirroring backend `AnalyzeRequest.layers`.
  - When set, it is forwarded to the backend in `useStreamingAnalysis` as `layers`.

- `AnalysisResult` and nested types:
  - No schema changes required for this iteration; we continue to use:
    - `layer3?: CommonMistake[]`
    - `layer4?: { relatedWords: RelatedWord[]; personalizedTip?: string }`
  - Ensure `RelatedWord` can handle additional items (up to 5) without UI overflow; `CognitiveScaffolding` already slices to available positions.

### 5.2 Streaming behavior (initial iteration)

- `useStreamingAnalysis`:
  - Continue to consume `layer1_*`, `layer2`, `layer3`, `layer4` events as today.
  - When we switch to deferred layers:
    - Start sending `layers` in the request payload:

      ```ts
      const payload = {
        word: request.word,
        context: request.context,
        page_type: request.pageType,
        learning_history: request.learningHistory,
        english_level: request.englishLevel,
        url: request.url,
        interests: ...,
        blocked_titles: ...,
        favorite_words: ...,
        layers: [2, 3, 4], // initial default, later configurable
      };
      ```

    - For an experiment where Common Mistakes are deferred, use `layers: [2, 4]` and let `CommonMistakes` fetch via `/api/analyze/mistakes`.

### 5.3 Component behavior

#### 5.3.1 CoachSummary (解读)

- Keep the display logic unchanged (render only when `personalizedTip` exists).
- Later improvement (out of strict scope for this spec, but enabled by new APIs):
  - Render a placeholder card immediately after Layer 1:
    - If `layer4` not yet present, show “正在为你整理解读…” with a subtle shimmer.
    - Once `/lexical-map/text` or SSE Layer 4 finishes, replace placeholder with real content.

#### 5.3.2 CognitiveScaffolding (Lexical Map)

- Now:
  - Uses `data.relatedWords.slice(0, positions.length)`; positions length = 4.
  - Warmup (`enableAsyncImageWarmup`) uses `/api/lexical-map/image`.
- After backend supports up to 5 related words:
  - Keep the UI limited to the first 4 for now; rely on `selectPreferredRelatedIndex` to pick good candidates for warmup.
  - No change to Lexi Learner warmup logic; image endpoint and caching remain unchanged (FR7).
- Future option (after two-stage backend is wired in):
  - When only candidate-level words are available (no `keyDifference`/`whenToUse` yet), show:
    - Node graph with labels.
    - A lightweight explanation like “点击某个节点加载详细区别”.
  - Once Stage B finishes, hydrate the `keyDifference` and `whenToUse` fields.

#### 5.3.3 CommonMistakes

- Initially:
  - Behavior stays the same; consumes `analysisResult.layer3`.
  - We may add a simple loading state if `isLoading` is true and `layer3` is missing.
- Deferred loading path (after `/analyze/mistakes` endpoint is available and stable):
  - If `analysisResult.layer3` is absent when the section mounts:
    - Show a compact skeleton or “点击生成常见错误”.
    - On click or visibility, call `/api/analyze/mistakes`.
    - Update `analysisResult.layer3` via `updateAnalysisLayer('layer3', ...)`.

## 6. Data Model & API Summary

### 6.1 Backend models (new/changed)

- `AnalyzeRequest`:
  - New optional field: `layers: Optional[list[int]]` (only 2, 3, 4 recognized; 1 is always streamed).

- New models:
  - `CommonMistakesRequest` (word, context, english_level?).
  - `LexicalMapTextRequest` (word, context, learning_history?, english_level?, interests?, blocked_titles?, favorite_words?).

- Existing models reused:
  - `Layer3Response`, `Layer4Response`, `CommonMistake`, `RelatedWord`, `InterestTopicPayload`.

### 6.2 API surface

- `/api/analyze` (POST, SSE) – unchanged URL & high-level behavior:
  - Request body: `AnalyzeRequest` (+ optional `layers`).
  - Events:
    - `layer1_chunk`, `layer1_complete`, `layer2`, `layer3`, `layer4`, `*_error`, `done`.
  - Now emits `layer2`/`3`/`4` in completion order rather than fixed order.

- `/api/analyze/mistakes` (POST, JSON):
  - Request: `CommonMistakesRequest`.
  - Response: `Layer3Response`.

- `/api/lexical-map/text` (POST, JSON):
  - Request: `LexicalMapTextRequest`.
  - Response: `Layer4Response`.

- `/api/lexical-map/image` (POST, JSON) – unchanged.

## 7. Delivery Phases

### Phase 1 – Backend-only latency improvements (low risk)

1. Add `layers` to `AnalyzeRequest` and support selective layer creation in `analyze_streaming`.
2. Fix sequential await by streaming layer 2/3/4 in completion order.
3. Remove `icon` from Layer 2 prompt/response and ignore it in `generate_layer2`.
4. Tighten Layer 3 & Layer 4 prompts for conciseness; reduce `max_tokens` for those calls.
5. Add per-layer model config fields in `Settings` and wire them into `LLMOrchestrator`.
6. Ensure all existing tests pass; add unit tests for:
   - `analyze_streaming` with different `layers` settings (mock LLM client).
   - `generate_layer2` ignoring `icon`.

### Phase 2 – New endpoints & optional deferred loading

1. Implement `/api/analyze/mistakes` and `/api/lexical-map/text` endpoints.
2. Add backend tests for these endpoints (FastAPI test client, mocked LLM).
3. Extend frontend `AnalysisRequest` to support `layers`.
4. Optionally run an experiment where:
   - Initial SSE request uses `layers=[2,4]` (skip Layer 3).
   - `CommonMistakes` fetches data via `/api/analyze/mistakes` on first view.

### Phase 3 – Lexical Map two-stage strategy (optional, stretch)

1. Implement `generate_layer4_candidates` + `enrich_layer4_from_candidates` and corresponding prompts.
2. Wire Stage A/B into `generate_layer4` so API surface stays unchanged but internal strategy is two-stage.
3. Optionally expose candidate-only SSE events / text endpoints and update frontend to:
   - Show Lexical Map nodes as soon as candidates are ready.
   - Fill in detailed differences and 解读 once Stage B completes.

## 8. Verification Plan

Backend:
- `cd backend && poetry run pytest -v`.
- `poetry run ruff check app/`.
- Manual checks:
  - With a running dev server, use `curl` or a small script to:
    - Call `/api/analyze` and confirm:
      - Layer 1 chunks stream as before.
      - Layer 2/3/4 events can arrive in any order.
    - Call `/api/analyze` with different `layers` payloads (e.g. `[2]`, `[2,4]`) and verify only corresponding events are emitted.
    - Call `/api/analyze/mistakes` and `/api/lexical-map/text` and validate JSON shapes.

Frontend:
- `cd extension && pnpm lint && pnpm typecheck`.
- `pnpm dev` with backend running:
  - Confirm:
    - Layer 1 appears within ~1–2 seconds.
    - 解读, Lexical Map, and Common Mistakes appear faster and can show partial content without blocking the rest of the UI.
    - Lexi Learner warmup behavior for manga images still works as before.

